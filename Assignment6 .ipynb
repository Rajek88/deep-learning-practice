{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Apply transfer learning with pre-trained VGG16 model on assignment 3 and analyze the result. \n",
        "\n",
        "Objectives:\n",
        "1. To learn pre-trained models\n",
        "2. To learn transfer learning\n",
        "Theory:\n",
        "Transfer learning\n",
        "Transfer learning (TL) is a research problem in machine learning (ML) that focuses on storing\n",
        "knowledge gained while solving one problem and applying it to a different but related problem. For\n",
        "example, knowledge gained while learning to recognize cars could apply when trying to recognize\n",
        "trucks. This area of research bears some relation to the long history of psychological literature\n",
        "on transfer of learning, although formal ties between the two fields are limited. From the practical\n",
        "standpoint, reusing or transferring information from previously learned tasks for the learning of new\n",
        "tasks has the potential to significantly improve the sample efficiency of a reinforcement\n",
        "learning agent.\n",
        "ResNet 50\n",
        "1. Use 3*3 filters mostly.\n",
        "2. Down sampling with CNN layers with stride 2.\n",
        "3. Global average pooling layer and a 1000-way fully-connected layer with Softmax in the\n",
        "end.\n",
        "Plain VGG and VGG with Residual Blocks\n",
        "There are two kinds of residual connections:\n",
        "Residual block\n",
        "1. The identity shortcuts (x) can be directly used when the input and output are of the\n",
        "same dimensions.\n",
        "Residual block function when input and output dimensions are same\n",
        "2. When the dimensions change, A) The shortcut still performs identity mapping, with extra zero\n",
        "entries padded with the increased dimension. B) The projection shortcut is used to match the\n",
        "dimension (done by 1*1 conv) using the following formula\n",
        "Residual block function when the input and output dimensions are not same.\n",
        "The first case adds no extra parameters, the second one adds in the form of W_{s}\n",
        "MOBILENET\n",
        "MobileNet is an efficient and portable CNN architecture that is used in real world applications.\n",
        "MobileNets primarily use depthwise seperable convolutions in place of the standard\n",
        "convolutions used in earlier architectures to build lighter models.MobileNets introduce two new\n",
        "global hyperparameters(width multiplier and resolution multiplier) that allow model developers to\n",
        "trade off latency or accuracy for speed and low size depending on their requirements.\n",
        "Architecture\n",
        "MobileNets are built on depthwise seperable convolution layers.Each depthwise seperable\n",
        "convolution layer consists of a depthwise convolution and a pointwise convolution.Counting\n",
        "depthwise and pointwise convolutions as seperate layers, a MobileNet has 28 layers.A standard\n",
        "MobileNet has 4.2 million parameters which can be further reduced by tuning the width multiplier\n",
        "hyperparameter appropriately.\n",
        "The size of the input image is 224 × 224 × 3.\n",
        "The detailed architecture of a MobileNet is given below :\n",
        "TYPE STRIDE KERNEL SHAPE INPUT SIZE\n",
        "Conv 2 3 × 3 × 3 × 32 224 × 224 × 3\n",
        "Conv dw 1 3 × 3 × 32 112 × 112 × 32\n",
        "Conv 1 1 × 1 × 32 × 64 112 × 112 × 32\n",
        "Conv dw 2 3 × 3 × 64 112 × 112 × 64\n",
        "Conv 1 1 × 1 × 64 × 128 56 × 56 × 64\n",
        "TYPE STRIDE KERNEL SHAPE INPUT SIZE\n",
        "Conv dw 1 3 × 3 × 128 56 × 56 × 128\n",
        "Conv 1 1 × 1 × 128 × 128 56 × 56 × 128\n",
        "Conv dw 2 3 × 3 × 128 56 × 56 × 128\n",
        "Conv 1 1 × 1 × 128 × 256 56 × 56 × 128\n",
        "Conv dw 1 3 × 3 × 256 28 × 28 × 256\n",
        "Conv 1 1 × 1 × 256 × 256 28 × 28 × 256\n",
        "Conv dw 2 3 × 3 × 256 28 × 28 × 256\n",
        "Conv 1 1 × 1 × 256 × 512 14 × 14 × 256\n",
        "Conv dw 1 3 × 3 × 512 14 × 14 × 512\n",
        "Conv 1 1 × 1 × 512 × 512 14 × 14 × 512\n",
        "Conv dw 1 3 × 3 × 512 14 × 14 × 512\n",
        "Conv 1 1 × 1 × 512 × 512 14 × 14 × 512\n",
        "Conv dw 1 3 × 3 × 512 14 × 14 × 512\n",
        "Conv 1 1 × 1 × 512 × 512 14 × 14 × 512\n",
        "Conv dw 1 3 × 3 × 512 14 × 14 × 512\n",
        "Conv 1 1 × 1 × 512 × 512 14 × 14 × 512\n",
        "Conv dw 1 3 × 3 × 512 14 × 14 × 512\n",
        "Conv 1 1 × 1 × 512 × 512 14 × 14 × 512\n",
        "Conv dw 2 3 × 3 × 512 14 × 14 × 512\n",
        "Conv 1 1 × 1 × 512 × 1024 7 × 7 × 512\n",
        "Conv dw 2 3 × 3 × 1024 7 × 7 × 1024\n",
        "Conv 1 1 × 1 × 1024 × 1024 7 × 7 × 1024\n",
        "Avg Pool 1 Pool 7 × 7 7 × 7 × 1024\n",
        "Fully Connected 1 1024 × 1000 1 × 1 × 1024\n",
        "Softmax 1 Classifier 1 × 1 × 1000\n",
        "Standard Convoltion layer :\n",
        "A single standard convolution unit (denoted by Conv in the table above) looks like this :\n",
        "Depthwise seperable Convolution layer\n",
        "A single Depthwise seperable convolution unit (denoted by Conv dw in the table above) looks\n",
        "like this :\n",
        "VGG16\n",
        "VGG16 is a convolutional neural network model proposed by K. Simonyan and A. Zisserman from the\n",
        "University of Oxford in the paper “Very Deep Convolutional Networks for Large-Scale Image\n",
        "Recognition”. The model achieves 92.7% top-5 test accuracy in ImageNet, which is a dataset of over\n",
        "14 million images belonging to 1000 classes. It was one of the famous model submitted to ILSVRC2014. It makes the improvement over AlexNet by replacing large kernel-sized filters (11 and 5 in the\n",
        "first and second convolutional layer, respectively) with multiple 3×3 kernel-sized filters one after\n",
        "another. VGG16 was trained for weeks and was using NVIDIA Titan Black GPU’s.\n",
        "The architecture depicted below is VGG16.\n",
        "VGG16 Architecture\n",
        "The input to cov1 layer is of fixed size 224 x 224 RGB image. The image is passed through a stack of\n",
        "convolutional (conv.) layers, where the filters were used with a very small receptive field: 3×3 (which\n",
        "is the smallest size to capture the notion of left/right, up/down, center). In one of the configurations,\n",
        "it also utilizes 1×1 convolution filters, which can be seen as a linear transformation of the input\n",
        "channels (followed by non-linearity). The convolution stride is fixed to 1 pixel; the spatial padding of\n",
        "conv. layer input is such that the spatial resolution is preserved after convolution, i.e. the padding is\n",
        "1-pixel for 3×3 conv. layers. Spatial pooling is carried out by five max-pooling layers, which follow\n",
        "some of the conv. layers (not all the conv. layers are followed by max-pooling). Max-pooling is\n",
        "performed over a 2×2 pixel window, with stride 2.\n",
        "Three Fully-Connected (FC) layers follow a stack of convolutional layers (which has a different depth\n",
        "in different architectures): the first two have 4096 channels each, the third performs 1000-way\n",
        "ILSVRC classification and thus contains 1000 channels (one for each class). The final layer is the softmax layer. The configuration of the fully connected layers is the same in all networks."
      ],
      "metadata": {
        "id": "tnSFlpUI233E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Code"
      ],
      "metadata": {
        "id": "kBdTLilX3A0j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "MekZBLlk221g",
        "outputId": "b1e18a71-40dc-4865-c0d7-c1fceeb82b26"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2bbfba4189b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfile_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdogorcat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models,Sequential,layers,preprocessing\n",
        "import os\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "file_names=os.listdir(\"/content/train\")\n",
        "dogorcat=[]\n",
        "for name in file_names:\n",
        "  category=name.split('.')[0]\n",
        "  if category=='dog':\n",
        "    dogorcat.append(\"DOG\")\n",
        "  else:\n",
        "    dogorcat.append(\"CAT\")\n",
        "    \n",
        "train=pd.DataFrame({'filename':file_names,'category':dogorcat})\n",
        "base=VGG16(include_top=False,input_shape=(224,224,3),weights='imagenet')\n",
        "base.trainable=False\n",
        "model=models.Sequential()\n",
        "model.add(base)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(120,activation=\"relu\"))\n",
        "model.add(layers.Dense(2,activation=\"softmax\"))\n",
        "model.compile(optimizer=\"adam\",loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image import ImageDataGenerator,load_img\n",
        "train_df,validate_df = train_test_split(train,test_size=0.1, random_state=42)\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "validate_df = validate_df.reset_index(drop=True)\n",
        "training = preprocessing.image.ImageDataGenerator(rotation_range=15, rescale=1/255, shear_range=0.1, zoom_range=0.2, horizontal_flip=True, width_shift_range=0.1, height_shift_range=0.1)\n",
        "total_train=train_df.shape[0]\n",
        "total_validate=validate_df.shape[0]\n",
        "trainingdata = training.flow_from_dataframe(train_df,\"/content/train\",x_col='filename',y_col='category',target_size=(224,224),class_mode='categorical',batch_size=4)\n",
        "validation = ImageDataGenerator(rescale=1./255)\n",
        "validationdata = validation.flow_from_dataframe(validate_df,\"/content/train\",x_col='filename',y_col='category',target_size=(224,224),class_mode='categorical',batch_size=4)\n",
        "model.fit(trainingdata,validation_data=validationdata,epochs=10,steps_per_epoch=total_train//200,validation_steps=total_validate//200)\n",
        "model.evaluate(validationdata)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "M4SvOSIv3lDj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}